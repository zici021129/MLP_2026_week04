{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00000-34f9557d-8f15-494e-8d65-74c8ae429c5c",
    "deepnote_cell_type": "markdown",
    "id": "gRJp4fGPvZ7c"
   },
   "source": [
    "# Week 4 - Regression and Model Evaluation\n",
    "\n",
    "**Student Name 1, Student Name 2**\n",
    "\n",
    "## Aims\n",
    "\n",
    "By the end of this notebook you will be able to \n",
    "\n",
    ">* fit a linear regression\n",
    ">* understand the basics of polynomial regression\n",
    ">* understand how to evaluate and compare models and select tuning parameters with training, validation, and testing.\n",
    "\n",
    "1. [Problem Definition and Setup](#setup)\n",
    "\n",
    "2. [Exploratory Data Analysis](#EDA)\n",
    "\n",
    "3. [Least Squares Estimation](#RBH)\n",
    "\n",
    "4. [Regression using scikit-Learn](#RSKL)\n",
    "\n",
    "5. [Polynomial Regression](#polyreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdHUSbWsvZ7h"
   },
   "source": [
    "During workshops, you will complete the worksheets together in teams of 2-3, using **pair programming**. From this week onwards, the worksheets will no longer contain cues to switch roles between driver and navigator; this should occur approximately every 15 minutes and should be more natural after the first weeks. When completing worksheets:\n",
    "\n",
    ">- You will have tasks tagged by (CORE) and (EXTRA). \n",
    ">- Your primary aim is to complete the (CORE) components during the WS session, afterwards you can try to complete the (EXTRA) tasks for your self-learning process. \n",
    "\n",
    "Instructions for submitting your workshops can be found at the end of worksheet. As a reminder, you must submit a pdf of your notebook on Learn by 16:00 PM on the Friday of the week the workshop was given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-645a25eb-6010-425a-88c0-ecf0093a9edc",
    "deepnote_cell_type": "markdown",
    "id": "6sVlUI4SvZ7i"
   },
   "source": [
    "---\n",
    "\n",
    "# Problem Definition and Setup <a id='setup'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEIja_2pvZ7n"
   },
   "source": [
    "## Packages\n",
    "\n",
    "First, let's load the packages you wil need for this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00002-d0af5d8f-8894-4c5a-b754-353993666790",
    "deepnote_cell_type": "code",
    "id": "KBfh_AXdvZ7o",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "# Data libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# sklearn modules and some other will be added later\n",
    "import sklearn\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV, KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mC7t8FfQvZ7p"
   },
   "source": [
    "## User Defined Helper Functions\n",
    "\n",
    "Below are two helper functions we will be using in this workshop. You can modify or create your own if you think it is useful or simply use already available functions within `sklearn`.  \n",
    "\n",
    "- `model_fit()`: Returns the mean squared error, root mean squared error and R^2 value of a fitted model based  on the provided X and y values with plotting as add-on.\n",
    "- `compute_std_error()`: Computes the standar error of the coefficients, with plotting as add-on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit(m, X, y, plot = False):\n",
    "    \"\"\"Returns the mean squared error, root mean squared error and R^2 value of a fitted model based \n",
    "    on provided X and y values.\n",
    "    \n",
    "    Args:\n",
    "        m: sklearn model object\n",
    "        X: model matrix to use for prediction\n",
    "        y: outcome vector to use to calculating rmse and residuals\n",
    "        plot: boolean value, should residual plots be shown \n",
    "    \"\"\"\n",
    "    \n",
    "    y_hat = m.predict(X)\n",
    "    MSE = mean_squared_error(y, y_hat)\n",
    "    RMSE = np.sqrt(mean_squared_error(y, y_hat))\n",
    "    Rsqr = r2_score(y, y_hat)\n",
    "    \n",
    "    Metrics = (round(MSE, 4), round(RMSE, 4), round(Rsqr, 4))\n",
    "    \n",
    "    res = pd.DataFrame(\n",
    "        data = {'y': y, 'y_hat': y_hat, 'resid': y - y_hat}\n",
    "    )\n",
    "    \n",
    "    if plot:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        plt.subplot(121)\n",
    "        sns.lineplot(x='y', y='y_hat', color=\"grey\", data =  pd.DataFrame(data={'y': [min(y),max(y)], 'y_hat': [min(y),max(y)]}))\n",
    "        sns.scatterplot(x='y', y='y_hat', data=res).set_title(\"Observed vs Fitted values\")\n",
    "        \n",
    "        plt.subplot(122)\n",
    "        sns.scatterplot(x='y_hat', y='resid', data=res).set_title(\"Fitted values vs Residuals\")\n",
    "        plt.hlines(y=0, xmin=np.min(y), xmax=np.max(y), linestyles='dashed', alpha=0.3, colors=\"black\")\n",
    "        \n",
    "        plt.subplots_adjust(left=0.0)\n",
    "        \n",
    "        plt.suptitle(\"Model (MSE, RMSE, Rsq) = \" + str(Metrics), fontsize=14)\n",
    "        plt.show()\n",
    "    \n",
    "    return MSE, RMSE, Rsqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_std_error(m, X, y, plot = False, feature_names = None, figsize = (5,5)):\n",
    "    \"\"\"Returns the standard errors of the model coefficients for a fitted linear model.\n",
    "    \n",
    "    Args:\n",
    "        m: sklearn LinearRegression model object or pipeline with LinearRegression as final step\n",
    "        X: training input matrix\n",
    "        y: training outcome vector\n",
    "        plot: boolean value, should coefficients be plotted with error bars\n",
    "        feature_names: list of feature names to use in the plot \n",
    "    \"\"\"\n",
    "    \n",
    "    y_hat = m.predict(X)\n",
    "    # Define feature matrix with intercept\n",
    "    X_ =  np.hstack([np.ones((X.shape[0], 1)), X]) \n",
    "    # If m is a pipeline, transform X accordingly\n",
    "    if isinstance(m, sklearn.pipeline.Pipeline): \n",
    "        X_ = np.hstack([np.ones((X.shape[0], 1)), m[:-1].transform(X)])\n",
    "    sigmsq = np.sum((y - y_hat)**2) / (X_.shape[0] - X_.shape[1]-1)\n",
    "    cov_mat = sigmsq * np.linalg.inv(X_.T@X_)\n",
    "    sd_er = np.sqrt(np.diag(cov_mat))\n",
    "\n",
    "    if plot:\n",
    "        if feature_names is None:\n",
    "            feature_names = m[:-1].get_feature_names_out() if isinstance(m, sklearn.pipeline.Pipeline) else m.feature_names_in_\n",
    "\n",
    "        coef = m[-1].coef_ if isinstance(m, sklearn.pipeline.Pipeline) else m.coef_\n",
    "        # Create data frame for plotting\n",
    "        df = pd.DataFrame({'feature': feature_names, 'coef': coef, 'std_err': 1.96*sd_er[1:]}).sort_values (\"coef\", ascending=False)\n",
    "\n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.bar(df['feature'], df['coef'])\n",
    "        plt.errorbar(df['feature'], df['coef'], yerr=df['std_err'], fmt=\"o\", color=\"r\")\n",
    "        #sns.barplot(data=df, y='feature', x='coef')\n",
    "        plt.title('Coefficient Estimates with 95% Confidence Intervals')\n",
    "        plt.xlabel('Features')\n",
    "        plt.ylabel('Coefficient Value')\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "    \n",
    "    return  sd_er, cov_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00003-90709695-8746-4669-9199-fd144a6ec872",
    "deepnote_cell_type": "markdown",
    "id": "yz3bjxcbvZ7r"
   },
   "source": [
    "## Data \n",
    "\n",
    "To begin, we will examine `insurance.csv` data set on the medical costs which comes from the [Medical Cost Personal dataset](https://www.kaggle.com/datasets/mirichoi0218/insurance). Our goal is to model the yearly medical charges of an individual using some combination of the other features in the data. The included columns are as follows:\n",
    "\n",
    "* `charges` - yearly medical charges in USD\n",
    "* `age` - the individuals age\n",
    "* `sex` - the individuals sex, either `\"male\"` or `\"female\"`\n",
    "* `bmi` - the body mass index of the individual\n",
    "* `children` - the number of dependent children the individual has\n",
    "* `smoker` - a factor with levels `\"yes\"`, the individual is a smoker and `\"no\"`, the individual is not a smoker\n",
    "\n",
    "We read the data into python using pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eF19U6ivvZ7r"
   },
   "outputs": [],
   "source": [
    "df_insurance = pd.read_csv(\"insurance.csv\")\n",
    "df_insurance.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izo4A3SSvZ7t"
   },
   "source": [
    "# Exploratory Data Analysis <a id='eda'></a>\n",
    "\n",
    "Before modelling, we will start with EDA to gain an understanding of the data, through descriptive statistics and visualizations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00005-da6d5f4b-bf6b-43f7-9751-a015e6a9924e",
    "deepnote_cell_type": "markdown",
    "id": "50AbKP76vZ7u"
   },
   "source": [
    "### ðŸš© Exercise 1 (CORE)\n",
    "\n",
    "a) Examine the data structure using `.info()` and look at the descriptive statistics using `.describe()`. What are the types of variables in the data set? Are they any missing values?\n",
    "\n",
    "b) Create a pairs plot of the data (make sure to color by `smoker` to visualize its influence). Describe any relationships you observe in the data. To better visualize the relationship between `children` and `charges`, create a violin plot (since `children` only takes a small number of integer values, many points are overlaid in the scatterplot and making visualization difficult).\n",
    "\n",
    "\n",
    "<details><summary><b><u>Hint</b></u></summary>\n",
    "    \n",
    "- You can use <code>sns.pairplot</code> and <code>sns.violinplot</code> with the hue argument to color by smoker.\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00006-4d9907ea-c7c7-4c16-aee7-5e12c3e00cc1",
    "deepnote_cell_type": "code",
    "id": "-mG-rvtrvZ7u",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "# Code for your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your ansher here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Train-Test Set <a id='gen'></a>\n",
    "\n",
    "Before modelling, we will first split the data into the train and test sets. This ensures that that we do not violate one of the golden rules of machine learning: never use the test set for training. As EDA can help guide the choice and form of model, we ideally want to split the data before EDA, to avoid peeking at the test data too much during this phase. However, in practice, we may need to investigate the entire data during EDA to get a better idea on how to handle issues such as missingness, categorical data (and rare categories), incorrect data, etc.      \n",
    "\n",
    "There are lots of ways of creating a test set. We will use a helpful function from `sklearn.model_selection` called `train_test_split`. You can have a look at the documentation: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html. Note that `train_test_split` defaults to randomly sampling the data to split it into training and validation/test sets, that is, the default value is `shuffle=True`  \n",
    "\n",
    "For reproducibility, we first fix the value in the numpy random seed about the state of randomness. This ensures that, every step including randomness, will produce the same output if we re-run the code or if someone else wants to reproduce our results (e.g. produce the same train-test split).\n",
    "\n",
    "In `sklearn`, the suggestion to control randomness across multiple consecutive executions is as follows: \n",
    "\n",
    "- In order to obtain reproducible (i.e. constant) results across multiple program executions, we need to remove all uses of `random_state=None`, which is the default.\n",
    "\n",
    "- Declare your own `rng` variable (random number generator) at the top of the program, and pass it down to any object that accepts a `random_state` parameter. You can check some details from here; https://numpy.org/doc/1.16/reference/generated/numpy.random.RandomState.html\n",
    "\n",
    "Thus, our first step before splitting the data is to define our `rng` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make this notebook's output identical at every run\n",
    "rng = np.random.seed(0)\n",
    "# might be good for our course\n",
    "# np.random.seed(11205) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 2 (CORE)\n",
    "\n",
    "Run the following code to use `train_test_split()` to split the data randomly into training (70%) and test (30%) sets. The `training set` will contain our training data, called `X_train` and `y_train`. The `test set` will contain our testing data, called `X_test` and `y_test`.\n",
    "\n",
    "Can you think on a scenario where not shuffling would be a good idea? What about when we do want to shuffle our data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a good practice first split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_insurance.drop('charges', axis = 1) # Set of features\n",
    "y = df_insurance['charges']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00008-7d871b29-2a08-4b7f-8421-d36b23056000",
    "deepnote_cell_type": "markdown",
    "id": "f8cnwbvKvZ7v"
   },
   "source": [
    "# Least Squares Estimation <a id='RBH'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00009-ca8096af-6bc2-4bf1-aea6-19d55952f208",
    "deepnote_cell_type": "markdown",
    "id": "wyQLrGCvvZ7w"
   },
   "source": [
    "Consider a linear regression model for `charges` using `bmi` and `smoker` as features in our model. Without sklearn functionalities, let's compute and visualize the least squares estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00010-92e05d0b-dedd-444b-a9f3-df423c4b0e70",
    "deepnote_cell_type": "markdown",
    "id": "JdkqCIkUvZ7w"
   },
   "source": [
    "### ðŸš© Exercise 3 (CORE)\n",
    "\n",
    "Create a scatter plot using `sns.scatterplot` of `charges` vs `bmi`, colored by `smoker`. Describe any apparent relationship between `charges` and `bmi` and comment on the difference between smokers and non-smokers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00011-abd9df3c-dd0b-43bc-a4b4-4369e7a9cc3e",
    "deepnote_cell_type": "code",
    "id": "UJt4fEW2vZ7w",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "# Code for your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 4 (CORE)\n",
    "\n",
    "Now, let's compute the least square estimates.\n",
    "\n",
    "First let's construct the feature matrix as an `np.array`. Recall that we need to include a column of ones to allow a non-zero intercept. We also need to convert the response `y_train` to an `np.array`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the design matrix\n",
    "X = np.c_[\n",
    "    np.ones(len(X_train)),\n",
    "    X_train.bmi,\n",
    "    X_train.smoker == \"yes\"\n",
    "]\n",
    "\n",
    "#Â Converting response training object to numpy array\n",
    "y = np.array(y_train)\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "a) Next, compute the least squares estimates $\\hat{\\boldsymbol{w}}$, using the expression from lectures and the `solve` function from `numpy.linalg`.\n",
    "\n",
    "b) What is the intercept for non-smokers and what is the intercept for smokers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00022-28a7a9d1-e95b-47b7-a31c-9b247c628087",
    "deepnote_cell_type": "markdown",
    "id": "mvrOq4afvZ7z"
   },
   "source": [
    "### ðŸš© Exercise 5 (CORE)\n",
    "\n",
    "a) Compute the fitted values from this model by calculating $\\hat{\\mathbf{y}} = \\boldsymbol{X} \\hat{\\boldsymbol{w}}$. \n",
    "\n",
    "b) Redraw your scatter plot of `charges`vs `bmi`, colored by `smoker` from Exercise 3, and overlay a line plot of the fitted values (fitted regression line) using `sns.lineplot`. Comment on the results and any potential feature engineering steps that could help to improve the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00023-04722d32-f420-4104-b25b-37656a71df76",
    "deepnote_cell_type": "code",
    "id": "3Mxs7ORhvZ71",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "# Code for your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 6 (EXTRA)\n",
    "\n",
    "In the following code following code, we compute the standard error for the predictions following the expression in lectures. Add dashed lines to your plot representing the prediction intervals for smokers and non-smokers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute standard error of predictions\n",
    "sigmsq = np.sum((y - yhat)**2) / (X.shape[0] - X.shape[1]-1)\n",
    "sd_err_pred  = np.array([np.sqrt(sigmsq * X[i,:] @ np.linalg.solve(X.T@X, X[i,:].T)) for i in range(X.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2uoEYNyw0Jfm"
   },
   "source": [
    "## Residuals\n",
    "\n",
    "A useful tool for evaluating a model is to examine the residuals of that model. For any standard regression model,  the residual for observation $n$ is defined as $y_n - \\hat{y}_n$ where $\\hat{y}_n$ is the model's fiited value for observation $n$. \n",
    "\n",
    "Studying the properties of the residuals is important for assessing the quality of the fitted regression model. This scatterplot (fitted vs residuals) gives us more intuition about the model performance. Briefly, \n",
    "\n",
    ">- If the normal linear model assumption is true then the residuals should be randomly scattered around zero with no discernible clustering or pattern with respect to the fitted values. \n",
    ">- Furthermore, this plot can be useful to check the constant variance (homoscedastic) assumption to see whether the range of the scatter of points is consistent over the range of fitted values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 7 (CORE)\n",
    "\n",
    "a) Calculate the residuals and create a residual plot (scatter plot of fitted vs residuals) for this model and color by smoker. Comment on quality of the model based on this plot.\n",
    "\n",
    "b) Compute the $R^2$ value for this model and comment on its value (recall from lectures that $R^2$ is 1 minus the sum of the squared residuals divided by the sum of squared differences between $\\mathbf{y}$ and its mean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yRdjYEgP1ahd"
   },
   "outputs": [],
   "source": [
    "# Part a: Code for your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pIICk-3l4TVj"
   },
   "source": [
    "_Part a: Type your answer here_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ClWZ7isN5ekk"
   },
   "outputs": [],
   "source": [
    "# Part b: Code for your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Part b: Type your answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00026-021075dd-c202-41db-9f49-2b4de4efd791",
    "deepnote_cell_type": "markdown",
    "id": "ilhDv3WHvZ73"
   },
   "source": [
    "---\n",
    "# Regression using scikit-Learn <a id='RSKL'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00027-068e26c7-da60-4c7c-8551-5f914b53af75",
    "deepnote_cell_type": "markdown",
    "id": "NtUQzMD9vZ73"
   },
   "source": [
    "Linear regression is available in **scikit-learn** (**sklearn**) through `LinearRegression` from the `linear_model` submodule. You can browse through the documentation and examples [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html). Let's start by importing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00028-40915b00-d692-4d82-9df3-1a72901e798d",
    "deepnote_cell_type": "code",
    "id": "WBf8UcpzvZ73",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00029-a3ead172-d8aa-4570-9aae-68cd6d18692d",
    "deepnote_cell_type": "markdown",
    "id": "o2W30cCUvZ73"
   },
   "source": [
    "In general sklearn's models are implemented by first creating a model object, and then using that object to fit your data. As such, we will now create a linear regression model object `lr` and use it to fit our data. Once this object is created we use the `fit` method to obtain a model object fitted to our data. \n",
    "\n",
    "_Note:_ by default an intercept is included in the model. So, we do NOT need to add a column of ones to our design matrix.\n",
    "\n",
    "_Note:_ in the code below we use `make_column_transformer` which automatically names all of our transformers. To see the names, run `preprocessor.named_transformers_`. You can then access each transformer by its name, if you want to view it's attributes or call methods on it, e.g. `preprocessor['onehotencoder'].get_feature_names_out()`.\n",
    "\n",
    "_Note:_ using the option `OneHotEncoder(drop=np.array(['Reference Category']))`, we can specify the which category to drop (the reference category) by replacing `'Reference Category'` with the desired category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00030-d0c72235-001d-4cd4-a50f-302b40d9e4d3",
    "deepnote_cell_type": "code",
    "id": "VcuBnGPavZ73",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "\n",
    "# Create feature matrix\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "preprocessor = make_column_transformer(\n",
    "    (OneHotEncoder(drop=np.array(['no'])), ['smoker']), # encode 'smoker' column\n",
    "    (StandardScaler(), ['bmi']), # scale 'bmi' column\n",
    "    remainder='drop', # drop everthing else\n",
    "    verbose_feature_names_out=False)\n",
    "X_train_ = preprocessor.fit_transform(X_train)\n",
    "\n",
    "lr_fit = lr.fit(\n",
    "    X = X_train_, \n",
    "    y = y_train\n",
    ")\n",
    "\n",
    "# To see the names of the transformers\n",
    "# preprocessor.named_transformers_\n",
    "# You can call a method or access attributes of each transformer by its name\n",
    "# preprocessor['onehotencoder'].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00031-b1d09da5-a8c6-46db-a5ec-617662e344a9",
    "deepnote_cell_type": "markdown",
    "id": "kMZfWvLKvZ74"
   },
   "source": [
    "This model object then has various useful methods and attributes, including `intercept_` and `coef_` which contain our estimates for $\\boldsymbol{w}$. \n",
    "\n",
    "_Note:_ if `fit_intercept=False` and a column of ones is included in the design matrix, then both the intercept and coefficient will be stored in `coef_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00032-d4cd3759-a588-4d95-b34e-507416ebc046",
    "deepnote_cell_type": "code",
    "id": "Rt0cuv8uvZ74",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "w_0 = lr_fit.intercept_  # Intercept term of the fitted model\n",
    "w_1 = lr_fit.coef_ \n",
    "\n",
    "w = np.concatenate([[w_0], w_1])\n",
    "feature_names = ['intercept'] + list(preprocessor.get_feature_names_out())  \n",
    "w_df = pd.DataFrame({'feature': feature_names, 'coefficient': w})\n",
    "w_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, to avoid _data leakage_ and improve _reproducibility_, we should always be combining our feature enginnering and model fitting steps into a single **pipeline**. \n",
    "\n",
    "_Note:_ in the code below we use `make_pipeline` which automatically names all of our transformers/estimators. To see the names, run `preprocessor.named_steps`. You can then access each step by its name or its index in the pipeline, e.g. `pipe['columntransformer'].get_feature_names_out()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine into pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "pipe = make_pipeline(\n",
    "    preprocessor,\n",
    "    LinearRegression()\n",
    ")\n",
    "\n",
    "pipe_fit = pipe.fit(\n",
    "    X = X_train,\n",
    "    y = y_train\n",
    ")\n",
    "\n",
    "# You can access the coefficients by the index of the final step\n",
    "pipe[-1].coef_ \n",
    "# Or by its name\n",
    "# pipe.named_steps\n",
    "# pipe['linearregression'].coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00035-9bb64eec-8879-48d5-832a-101c7a3dc752",
    "deepnote_cell_type": "markdown",
    "id": "lLUKEr2YvZ75"
   },
   "source": [
    "The model fit objects also provides additional useful methods for evaluating the model $R^2$ (`score`) and calculating predictions (`predict`). Let's use the latter to compute the fitted values and predictions, as well as some metrics to evaluate the performance on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00036-fdd889c3-0b7b-449e-a815-6e1e055f9703",
    "deepnote_cell_type": "code",
    "id": "rqv-rPpQvZ75",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "# Fitted values\n",
    "y_fit = pipe.predict(X_train)\n",
    "\n",
    "# Predicted values\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "# The mean squared error of the training set \n",
    "print(\"Training Mean squared error: %.3f\" % mean_squared_error(y_train, y_fit))\n",
    "# The R squared of the training set \n",
    "print(\"Training R squared: %.3f\" % r2_score(y_train, y_fit))\n",
    "\n",
    "# The mean squared error of the test set \n",
    "print(\"Test Mean squared error: %.3f\" % mean_squared_error(y_test, y_pred))\n",
    "# The R squared of the test set \n",
    "print(\"Test R squared: %.3f\" % r2_score(y_test, y_pred))\n",
    "\n",
    "# Another way for R2 calculation\n",
    "print(pipe.score(X_train, y_train)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can use our helper function and also plot the fitted values and residuals to identify if there are potential patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we use the pre-defined function\n",
    "model_fit(pipe, X_train, y_train, plot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can use our helper function to compute standard errors of the coefficients and plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd, cov_mat = compute_std_error(pipe, X_train, y_train, plot=True, feature_names=['smoker_yes', 'bmi'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 8 (CORE)\n",
    "\n",
    "Next, modify your pipeline to also include `age` as feature.\n",
    "\n",
    "Fit the model on the training data and calculate performance metrics similar to above ($R^2$, and MSE / RMSE). How does this model compare to previous one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "# Polynomial Regression<a id='polyreg'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial features in sklearn\n",
    "\n",
    "sklearn has a built in function called `PolynomialFeatures` which can be used to simplify the process of including polynomial features in a model. You can browse the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html). This function is included in the *preprocessing* module of sklearn, as with other python functions we can import it as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construction and use of this is similar to what we have already seen with other transformers; we construct a PolynomialFeatures object in which we set basic options (e.g. the degree of the polynomial) and then apply the transformation to our data by calling `fit_transform`. This will generate a new model matrix which includes the polynomial features up to the degree we have specified.\n",
    "\n",
    "Run the following code for a simple illustration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 3, 4])\n",
    "PolynomialFeatures(degree = 2).fit_transform(x.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when we use this transformation, we get **all of the polynomial transformations of x from 0 to degree**. \n",
    "\n",
    "In this case, the **0 degree column** is equivalent to **the intercept column**. If we do not want to include this we can construct `PolynomialFeatures` with the option `include_bias=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PolynomialFeatures(degree = 2, include_bias=False).fit_transform(x.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `PolynomialFeatures` to add only interaction terms, through the option `interaction_only=True`. As an illustration run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1, 2, 3, 4],[0,0,1,1]]).T\n",
    "PolynomialFeatures(interaction_only=True,include_bias=False).fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactions\n",
    "\n",
    "Now, let's create a pipeline that:\n",
    "\n",
    "- includes `bmi` as a numerical variable and smoker condition as a categorical variable\n",
    "\n",
    "- applies encoding for the `smoker` variable\n",
    "\n",
    "- uses `PolynomialFeatures` to include an **interaction** between `smoker` and `bmi`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pipeline for model that includes interactions\n",
    "preprocessor = make_column_transformer(\n",
    "    (OneHotEncoder(drop=np.array(['no'])), ['smoker']), # encode 'smoker' column\n",
    "    (StandardScaler(), ['bmi']), # scale 'bmi' column\n",
    "    verbose_feature_names_out=False)\n",
    "\n",
    "# Create and fit new pipeline\n",
    "pipe3 = make_pipeline(\n",
    "    preprocessor,\n",
    "    PolynomialFeatures(interaction_only=True,include_bias=False), # add extra step for interaction terms\n",
    "    LinearRegression()\n",
    ").fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note:_ we set `include_bias=False` as the intercept is included in linear regression by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also extract the **names of the features** using the method `get_feature_names_out()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names after the first transformer\n",
    "print(pipe3[0].get_feature_names_out())\n",
    "\n",
    "# Names after the first transformer and interaction step\n",
    "print(pipe3[:-1].get_feature_names_out())\n",
    "\n",
    "# Create a data frame to see the intercept and coefficients\n",
    "w = [pipe3[-1].intercept_] + pipe3[-1].coef_.tolist() \n",
    "feature_names = ['intercept'] + list(pipe3[:-1].get_feature_names_out())  \n",
    "w_df = pd.DataFrame({'feature': feature_names, 'coefficient': w})\n",
    "w_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 9 (CORE)\n",
    "\n",
    "For the model trained above (`pipe3`):\n",
    "\n",
    "a) What is the intercept and slope for non-smokers and what is the intercept and slope for smokers?\n",
    "\n",
    "b) Compute the fitted values by calling `predict`. Draw the scatter plot of bmi against charges, colored by smoker (from Exercise 3), and overlay a line plot of the fitted values (fitted regression lines).\n",
    "\n",
    "c) How does this model compare to our previous models `pipe` and `pipe2`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for your answer here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's explore including nonlinearity into the model through a polynomial basis function expansions of `bmi`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 10 (CORE)\n",
    "\n",
    "First, suppose you naively apply a polynomial basis function expansion to the feature matrix containing `bmi` and `smoker`: \n",
    "\n",
    "- Run the code below to create the transformed feature matrix using `PolynomialFeatures` assuming `degree=2`. \n",
    "\n",
    "- Why should we **NOT** use this naive polynomial basis function expansion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transformed feature matrix containing a polynomial expansion (degree 2) of bmi and smoker \n",
    "\n",
    "# Create Pipeline for model that includes interactions\n",
    "preprocessor = make_column_transformer(\n",
    "    (OneHotEncoder(drop=np.array(['no'])), ['smoker']), # encode 'smoker' column\n",
    "    (StandardScaler(), ['bmi']), # pass through 'bmi' column\n",
    "    verbose_feature_names_out=False)\n",
    "\n",
    "# Create and fit new pipeline\n",
    "fe = make_pipeline(\n",
    "    preprocessor,\n",
    "    PolynomialFeatures(degree=2), # add step for polynomial features\n",
    ")\n",
    "\n",
    "Xt = fe.fit_transform(X_train)\n",
    "Xt_ = pd.DataFrame(Xt, columns=fe.get_feature_names_out())\n",
    "Xt_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 11 (CORE)\n",
    "\n",
    "Now, let's create a model that allows nonlinearity of `bmi` through polynomial basis function expansion of degree 3 (with no interactions for ease of exposition). \n",
    "\n",
    "Create a new pipeline to construct this model. Train this model and then plot the fitted regression line and compute the performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the Order of the Polynomial\n",
    "\n",
    "How can we choose the order of the polynomial? \n",
    "\n",
    "In lecture, we discussed how chosing the degree to be too large can cause over fittting. When we over fit a polynomial regression model, the MSE for the training data will appear to be low which might indicate that the model is a good fit. But, as a result of over fitting, the MSE for the predictions of the unseen test data may begin to increase. However, we can **NOT** use the test to determine the order of the polynomial, so in the following, we explore using cross-validation to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tunning with GridSearchCV\n",
    "\n",
    "If we wish to test over a specific set of parameter values using cross validation we can use the `GridSearchCV` function from the `model_selection` submodule. In this setting, the hyperparamer is actually the degree of the polynomial that we are investigating. \n",
    "\n",
    "This argument is a dictionary containing parameters names as keys and lists of parameter settings to try as values. Since we are using a pipeline, our parameter name will be the name of the pipeline step, `columntransformer`, followed by `__`, (then, the name of next step if applicable, e.g. `pipeline__` since we are using `Pipeline` for the steps of the numerical features, and `polynomialfeatures__` for the polynomial expansion) , and then the parameter name, `degree`. So for our pipeline the parameter is named `columntransformer__pipeline__polynomialfeatures__degree`. If you want to list the names of all available parameters you can call the `get_params()` method on the model object, e.g. `polyreg_pipe.get_params()` here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pipeline for model that includes polynomial expansions of bmi\n",
    "num_pre = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    PolynomialFeatures(include_bias=False)\n",
    ")\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (OneHotEncoder(drop=np.array(['no'])), ['smoker']), # encode 'smoker' column\n",
    "    (num_pre, ['bmi']), # polynomial features of 'bmi' column\n",
    "    verbose_feature_names_out=False)\n",
    "\n",
    "# Create the new pipeline\n",
    "polyreg_pipe = make_pipeline(\n",
    "    preprocessor,\n",
    "    LinearRegression(), # add step for polynomial features\n",
    ")\n",
    "\n",
    "# To see the names of the parameters that can be tuned\n",
    "# polyreg_pipe.get_params() #names of all parameters\n",
    "# polyreg_pipe['columntransformer'].get_params()  #names of parameters for the preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for degree of polynomial features\n",
    "parameters = {\n",
    "    'columntransformer__pipeline__polynomialfeatures__degree': np.arange(1,10,1)\n",
    "}\n",
    "\n",
    "# Define cross-validation scheme\n",
    "kf = KFold(n_splits = 5, shuffle = True, random_state=rng)\n",
    "\n",
    "# Grid search with cross-validation\n",
    "grid_search = GridSearchCV(polyreg_pipe, parameters, cv = kf, \n",
    "                           scoring = 'neg_mean_squared_error', return_train_score=True).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code goes through the process of fitting all $5 \\times 9$ models as well as storing and ranking the results for the requested scoring metric(s). Note that here we have used `neg_mean_squared_error` as our scoring metric which returns the **negative of the mean squared error**. For more on metrics of regression models, please see: https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics\n",
    "\n",
    "- As the name implies this returns the negative of the usual fit metric, this is because sklearn expects to always optimize for the maximum of a score and the model with the largest negative MSE will therefore be the \"best\". \n",
    "\n",
    "- In this workshop we have used MSE as a metric for testing our models. This metric is entirely equivalent to the root mean squared error for purposes of ranking / ordering models (as the square root is a monotonic transformation). \n",
    "- Sometimes the RMSE is prefered as it is more interpretable, because it has the same units as $y$.\n",
    "\n",
    "Once all of the submodels are fit, we can determine the optimal hyperparameter value by accessing the object's `best_*` attributes,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"best index: \", grid_search.best_index_)\n",
    "print(\"best param: \", grid_search.best_params_)\n",
    "print(\"best score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best estimator is stored in the `.best_estimator` attribute. By default, after this model is found, it is retrained on all training data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_[-1].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the names of the features and coefficients\n",
    "w = [grid_search.best_estimator_[-1].intercept_] + grid_search.best_estimator_[-1].coef_.tolist() \n",
    "feature_names = ['intercept'] + list(grid_search.best_estimator_[:-1].get_feature_names_out())  \n",
    "w_df = pd.DataFrame({'feature': feature_names, 'coefficient': w})\n",
    "w_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute fitted values\n",
    "yhat =grid_search.predict(X_train)\n",
    "\n",
    "# Plot fitted values\n",
    "ax = sns.scatterplot(x = X_train.bmi, y = y_train, hue = X_train.smoker)\n",
    "sns.lineplot(x = X_train.bmi, y = yhat, hue = X_train.smoker, ax=ax, legend = False)\n",
    "ax.set(ylabel='charges')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross-validated scores are stored in the attribute `cv_results_`. This contains a number of results related to the grid search and cross-validation. We can convert it into a pandas data frame to view the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also recommend to plot the CV scores. Although the grid search may report a best value for the parameter corresponding to the maximum CV score (e.g. min CV MSE), if the curve is relatively flat around the minimum, we may prefer the simpler model.\n",
    "\n",
    "Note in this case, I have also used the option `return_train_score=True` in `GridSearchCV()`, in order to save also the training scores. As expected training MSE decreases when increasing the degree of the polynomial, but the CV MSE has more of a U-shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = np.arange(1,10,1)\n",
    "fig, ax = plt.subplots(1,1,figsize=(6,4))\n",
    "plt.scatter(degree,-grid_search.cv_results_['mean_train_score'], color='k')\n",
    "plt.plot(degree,-grid_search.cv_results_['mean_train_score'], color='k', label='Mean Train MSE')\n",
    "plt.scatter(degree,-grid_search.cv_results_['mean_test_score'], color='r')\n",
    "plt.plot(degree,-grid_search.cv_results_['mean_test_score'], color='r', label='CV MSE')\n",
    "ax.legend()\n",
    "ax.set_xlabel('degree')\n",
    "ax.set_ylabel('MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 12 (CORE)\n",
    "\n",
    "Based on the plot above, would you use the best estimator or choose a different degree? Why?\n",
    "\n",
    "Try changing `random_state` in the grid search to different value. Does the best estimator still choose the same degree?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 13 (EXTRA)\n",
    "\n",
    "Try an alternative model of your choice. What have you chosen and why? Are there any parameters to tune?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "_Type your answere here_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pipeline for model that includes polynomial expansions of bmi\n",
    "num_pre = make_pipeline(\n",
    "    StandardScaler()\n",
    ")\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (OneHotEncoder(drop=np.array(['no'])), ['smoker']), # encode 'smoker' column\n",
    "    (num_pre, ['bmi']), # polynomial features of 'bmi' column\n",
    "    verbose_feature_names_out=False)\n",
    "\n",
    "# Create the new pipeline\n",
    "polyreg_pipe2 = make_pipeline(\n",
    "    preprocessor,\n",
    "    PolynomialFeatures(interaction_only=True,include_bias=False), # add step for interaction terms,\n",
    "    make_column_transformer(\n",
    "        ('passthrough', [0]),\n",
    "        (PolynomialFeatures(include_bias=False), [1]),\n",
    "        (PolynomialFeatures(include_bias=False), [2]),\n",
    "        verbose_feature_names_out=False\n",
    "    ),\n",
    "    LinearRegression(), # add step for polynomial features\n",
    ").fit(X_train,y_train)\n",
    "\n",
    "#polyreg_pipe2[:-1].get_feature_names_out()\n",
    "#polyreg_pipe2.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid for degrees of polynomial features\n",
    "parameters = {\n",
    "    'columntransformer-2__polynomialfeatures-1__degree': np.arange(1,5,1),\n",
    "    'columntransformer-2__polynomialfeatures-2__degree': np.arange(1,5,1)\n",
    "}\n",
    "\n",
    "# Cross-validation scheme\n",
    "kf = KFold(n_splits = 5, shuffle = True, random_state=rng)\n",
    "\n",
    "# Grid search with cross-validation\n",
    "grid_search2 = GridSearchCV(polyreg_pipe2, parameters, cv = kf, \n",
    "                            scoring = 'neg_mean_squared_error', return_train_score=True\n",
    "                            ).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"best param: \", grid_search2.best_params_)\n",
    "print(\"best score: \", grid_search2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print coefficients of the best estimator\n",
    "w = [grid_search2.best_estimator_[-1].intercept_] + grid_search2.best_estimator_[-1].coef_.tolist() \n",
    "feature_names = ['intercept'] + list(grid_search2.best_estimator_[:-1].get_feature_names_out())  \n",
    "w_df = pd.DataFrame({'feature': feature_names, 'coefficient': w})\n",
    "w_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute fitted values\n",
    "yhat = grid_search2.predict(X_train)\n",
    "\n",
    "# Plot fitted values\n",
    "ax = sns.scatterplot(x = X_train.bmi, y = y_train, hue = X_train.smoker)\n",
    "sns.lineplot(x = X_train.bmi, y = yhat, hue = X_train.smoker, ax=ax, legend = False)\n",
    "ax.set(ylabel='charges')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit(grid_search2.best_estimator_, X_train, y_train, plot = True)\n",
    "model_fit(grid_search2.best_estimator_, X_test, y_test, plot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further resources\n",
    "\n",
    "- About common pitfalls and interpreting coefficients: \n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/inspection/plot_linear_model_coefficient_interpretation.html#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competing the Worksheet\n",
    "\n",
    "At this point you have hopefully been able to complete all the CORE exercises and attempted the EXTRA ones. Now \n",
    "is a good time to check the reproducibility of this document by restarting the notebook's\n",
    "kernel and rerunning all cells in order.\n",
    "\n",
    "Before generating the PDF, please **change 'Student 1' and 'Student 2' at the top of the notebook to include your name(s)**.\n",
    "\n",
    "Once that is done and you are happy with everything, you can then run the following cell \n",
    "to generate your PDF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to pdf mlp_week04_key.ipynb "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "title": "MLPy Workshop 4"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
